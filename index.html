<!DOCTYPE html>
<html lang="en-US">
<script id="allow-copy_script">
(function agent() {
let unlock = false
document.addEventListener('allow_copy', (event) => {
unlock = event.detail.unlock
})

const copyEvents = [
'copy',
'cut',
'contextmenu',
'selectstart',
'mousedown',
'mouseup',
'mousemove',
'keydown',
'keypress',
'keyup',
]
const rejectOtherHandlers = (e) => {
if (unlock) {
  e.stopPropagation()
  if (e.stopImmediatePropagation) e.stopImmediatePropagation()
}
}
copyEvents.forEach((evt) => {
document.documentElement.addEventListener(evt, rejectOtherHandlers, {
  capture: true,
})
})
})()
</script>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


<title>Prof. Zheng Zhang's Homepage</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="Zheng Zhang@HITSZ">
<meta property="og:locale" content="en_US">

<link rel="shortcut icon" href="asset/favicon.png" />
<link rel="preconnect" href="https://fonts.gstatic.com/">
<link rel="preload" href="./asset/css" as="style" type="text/css" crossorigin="">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#708090">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<link rel="stylesheet" href="./asset/style.css">

</head>
<body>
<!-- Nevigation -->
<div id="c-nav" class="c-nav">
<div class="container navFlex">
<div class="flexItem">
  <img id='logo' class="logo" src="./asset/home.png" />
</div>
<!-- <div class="flexItem hiden">
  <img class="btnImg" src="./asset/List.png" />
</div> -->
<div class="flexItem show">
  <ul>
    <li><a id="BiographyNav">Biography</a></li>
    <!-- <li><a id="RecentNav">What's New</a></li> -->
    <li><a id="PublicationsNav">Publications</a></li>
    <li><a id="ProfessionalNav">Professional Activities</a></li>
    <li><a id="GroupNav">Team</a></li>
  </ul>
</div>
</div>
</div>

<main id="content" class="main-content" role="main">
<div class="main-container">
<div class="content-container">
  <div class="content-container-left">
    <span class="blue_2">
	<font size = "5" color="00008B" face = "Verdana">Zheng Zhang</font>
    </span><br>
    Professor, Doctoral Supervisor<br>
    Cheung Kong Young Scholar (教育部青年长江学者)<br>
    Harbin Institute of Technology, Shenzhen, China<br>
    <a href="https://scholar.google.com/citations?hl=en&user=tpVOb2EAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">[Google Scholar]</a>
    <a href="http://faculty.hitsz.edu.cn/zhangzheng" target="_blank">[Homepage in Chinese]</a>
	<br><br>
    <span class="blue_2">
	Area Chair: ICML/NeurIPS/ICLR/CVPR/MM<br>
	Associate Editor: IEEE TIFS/IEEE TAFFC/IEEE JBHI
    </span>
  </div> 
  <div class="content-container-right">
    <img src="./asset/profile.jpg" alt="Drawing" style="
    max-height: 220px;
    border: 1px solid #ccc;
    border-radius: 5px;
    -moz-border-radius: 15px;
    -khtml-border-radius: 15px;
    -webkit-border-radius: 15px;">
  </div>
</div> 
	
<div id="Faculty"></div>
<p style="text-align:justify; text-justify:inter-ideograph;">
Dr. Zheng Zhang is a faculty member at School of Computer Science and Technology, 
Harbin Institute of Technology, Shenzhen, China, and also holds an adjunct position 
at Peng Cheng Laboratory, Shenzhen, China. He is leading the Sustainable Multimodal Learning (<a href="https://cszhengzhang.cn/SMULL" target="_blank"><b><font color="ad2e24">S</font><font color="#FBBC05">M</font><font color="#4285F4">U</font><font color="#34A853">L</font><font color="#EA4335">L</font></b></a>)
Research Group, Harbin Institute of Technology, Shenzhen, and also the deputy director of the Shenzhen Key Laboratory of Visual 
Object Detection and Recognition, Shenzhen, China.<br><br>
<b><font color="#660000">Openings</font></b>: <font color="1b263b">I am continuously looking for highly motivated Ph.D. students 
and postdoctoral researchers to work on machine learning, computer vision, and multimedia. Please send me your CV if interested. 
I can supervise Ph.D. students affiliated with the Harbin Institute of Technology as well as Peng Cheng Laboratory. 
You also may refer to my SMULL research group for detailed information.</font><br>
<b><img class="btnImg" src="./asset/fire.png" style=" max-height: 23px;"/> <font color="780000" face = "Verdana">招生</font></b>：<font color="#163460" face = "Verdana">欢迎申报<b><font color="ad2e24">S</font><font color="#FBBC05">M</font><font color="#4285F4">U</font><font color="#34A853">L</font><font color="#EA4335">L</font></b>研究组2026年硕士/博士研究生！</font><br>
<ul>
<li>Email: <u>darrenzz219@gmail.com</u> (personal/research/review invitation) <b>&</b> <u>zhengzhang@hit.edu.cn</u> (teaching/admission)</li>
<li>Office: Room 1523, Building L, HIT Campus, University Town of Shenzhen, Shenzhen 518055, China</li>
</p>
<font color="00008B">"Happiness depends upon ourselves." — Aristotle</font><br>
<font color="191970">"Everything should be made as simple as possible, but not simpler."   — Albert Einstein</font><br>
<font color="660000">"Not everything that counts can be counted, and not everything that’s counted truly counts."  — Albert Einstein</font>
</p>
</ul>

<div id="Biography"></div><br>
	<p style="margin-bottom: 15px"><font size = "4.5" face = "Verdana" color="Brown"><b>Biography</b></font></p>
<p style="text-align:justify; text-justify:inter-ideograph;">
	Zheng Zhang received his Ph.D. from the Harbin Institute of Technology (HIT), supervised by Prof. <a href="https://scholar.google.com/citations?user=zOVgYQYAAAAJ&hl=en" target="_blank">Yong Xu</a> (Changjiang Professorship). 
	During his Ph.D., he visited the Institute of Automation of Chinese Academy of Sciences, advised by Prof. <a href="https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=en" target="_blank">Cheng-Lin Liu</a> (IEEE Fellow). 
	Following his doctoral studies, he joined The Hong Kong Polytechnic University (PolyU) as an Assistant Researcher and later became a Postdoctoral Research Fellow 
	in the Data Science Group at The University of Queensland (UQ), Australia, supervised by Prof. <a href="https://scholar.google.com/citations?user=iAWMsgEAAAAJ&hl=en" target="_blank">Helen Huang</a> (IEEE Fellow).
	He was also fortunately mentored by Prof. <a href="https://scholar.google.com/citations?user=krryaDkAAAAJ&hl=en" target="_blank">Heng Tao Shen</a> (Member of Academia Europaea, ACM/IEEE Fellow) 
	and Prof. <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en" target="_blank">Ling Shao</a> (IEEE Fellow).
	Since 2019, Dr. Zhang has been a faculty member at the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China.
</p>
<p style="text-align:justify; text-justify:inter-ideograph;">
	Dr. Zhang's research interests include Machine Learning and Multimedia, with particular emphasis on multimodal learning, resource-efficient deep learning, and AI security. 
	He has published over 100 papers in leading international journals and conferences. He has received prestigious recognitions such as the Distinguished Ph.D. Dissertation Award from CIE (2019), 
	Outstanding Young Research Achievement Award from CAAI (2019), Excellent Young Scientists Fund from Shenzhen (2023), and Pearl River Scholar Program of Guangdong Province (2023). 
	His innovative multimedia systems have been featured by prominent media outlets such as UN COP26, Xinhua News,
	and have been successfully transferred to leading industries like Tencent, Alibaba Group, Huawei, ICBC, and SZIDC. 
	He has been recognized among the 'World’s Top 2% Scientists' for several consecutive years and is a Senior Member of IEEE and CCF.
</p>	
<p style="text-align:justify; text-justify:inter-ideograph;">
      Dr. Zhang serves/served on the editorial boards of several prominent journals, including IEEE Trans. on Information Forensics & Security (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206" target="_blank">IEEE TIFS</a>), 
	IEEE Trans. on Affective Computing (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5165369" target="_blank">IEEE TAFFC</a>), 
	IEEE Journal of Biomedical and Health Informatics (<a href="https://www.embs.org/jbhi/" target="_blank">IEEE JBHI</a>), and Elsevier journals. 
	He has regularly contributed as an Area Chair for top-tier AI conferences, such as ICML, NeurIPS, CVPR, ICLR, ACM MM, etc. 
	Moreover, he has played key roles in organizing international conferences, including ADMA 2021 and 2023, ACM Multimedia Asia 2021, and other significant events.
</p>

<div id="Research Interests"></div>
    <p style="margin-bottom: 15px"><font size = "4.5" face = "Verdana" color="Brown"><b>Research Interests</b></font></p>
<ul>
	<li> Efficient Deep Learning, PEFT, AI Security</li>
	<li> Cross-media Intelligence, Retrieval, Big Data Analysis</li>
	<li> Trustworthy Multimodal Learning, Uncertainty Modeling</li>
	<li> AI in Healthcare, Medical Image Analysis</li>
</ul> 

<div id="Publications"></div> <br>
    <p style="margin-bottom: 15px"><font size = "4.5" face = "Verdana" color="Brown"><b>Recent Publications</b></font>
	    [<a href="https://cszhengzhang.cn/SelPub/" target="_blank">Selected Pub</a>]
	    [<a href="https://scholar.google.com/citations?hl=en&user=tpVOb2EAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Full Pub</a>]</p>
<div style="text-align:justify;text-justify:inter-ideograph">
	<b>Books:</b></br>
	<ol class="work-list" reversed>
		<li style="margin-bottom: 10px">Zheng Zhang, Binary Representation Learning on Visual Images, ISBN: 978-981-97-2111-5, Springer Nature, Jun. 2024. [<a href="https://link.springer.com/book/10.1007/978-981-97-2112-2" target="_blank">Link</a>]</li>
		<li style="margin-bottom: 10px">Zheng Zhang, Yong Xu, Guangming Lu, Structural Representation Learning for Data Analysis, Posts & Telecom Press, China, ISBN: 978-7-115-58401-4, 2022. [<a href="https://item.jd.com/13520887.html" target="_blank">Link</a>]</li>
		<li style="margin-bottom: 10px">张海军, 马江虹, 张正, 数据结构与智能算法, 清华大学出版社, ISBN: 978-7-302-68438-1, 2025. (本科生教材) </li>
		<li style="margin-bottom: 10px">Lei Zhu, Jingjing Li, Zheng Zhang, Dynamic Graph Learning for Dimension Reduction and Data Clustering, 
			Synthesis Lectures on Computer Science (SLCS), ISBN: 978-3-031-42312-3, Springer Nature, 2023.</li>
		<li style="margin-bottom: 10px">Xiaochun Yang, Chang-Dong Wang, Saiful Islam, Zheng Zhang (Eds.), 
			The 16th International Conference on Advanced Data Mining and Applications, ADMA 2020, Foshan, China, Nov. 12-14 2020, Springer LNAI, vol. 12447, ISBN: 978-3-030-65389-7, 2020.</li>
		<li style="margin-bottom: 10px">Shuihua Wang, Zheng Zhang, Yuan Xu (Eds.), The IoT and Big Data Technologies for Health Care, 
			The second EAI International Conference, IoTCARE 2021, October 18-19, 2021, Springer LNICS, Social Informatics and Telecommunications Engineering, 2021.</li>
	</ol>
	
	<br><b>Journal paper</b>:</br>
	<ol class="work-list" reversed>
		<li style="margin-bottom: 10px">T. Wang, F. Li, L. Zhu, J. Li, Z. Zhang, H. T. Shen, Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions,
			<i>Proceedings of the IEEE (<b>PIEEE</b>)</i>, vol. 112, no. 11, pp. 1716-1754, <b>2025</b>. [<a href="https://ieeexplore.ieee.org/document/10843094" target="_blank">Link</a>][<a href="https://arxiv.org/abs/2308.14263v3" target="_blank">Paper</a>][<a href="https://cross-modal-retrieval.github.io/" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">W. He, Z. Zhang<sup>&#9993</sup>, X. Zhu, Dual Correlation Guided Anchor Learning for Scalable Incomplete Multi-view Clustering,
			<i>IEEE Transactions on Neural Networks and Learning System (<b>TNNLS</b>)</i>, DOI: 10.1109/TNNLS.2025.3562297, <b>2025</b>. [<a href="https://ieeexplore.ieee.org/document/xxx" target="_blank">Link</a>][<a href="https://github.com/DarrenZZhang/TNNLS25-DCGA" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">X. Guan&sup1;, Z. Zhang&sup1;, Y. Wang, Y. Li, Y. Zhang, Supervised Information Mining from Weakly Paired Images for Breast IHC Virtual Staining, 
			<i>IEEE Transactions on Medical Imaging (<b>TMI</b>)</i>, <b>2025</b>. [<a href="https://ieeexplore.ieee.org/document/10820871" target="_blank">Link</a>][<a href="https://github.com/xianchaoguan/SIM-GAN" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">G. Xie, J. Li, T. Guo, X. Shu<sup>&#9993</sup>, F. Zhao, Z. Zhang<sup>&#9993</sup>, L. Shao, Attribute Prompt Alignment Network for Zero-Shot Learning, 
			<i>IEEE Transactions on Neural Networks and Learning System (<b>TNNLS</b>)</i>, <b>2025</b>. [<a href="https://ieeexplore.ieee.org/document/xxx" target="_blank">Link</a>][<a href="https://github.com/APAN-Anonymous/APAN" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">Z. Zhang<sup>&#9993</sup>, X. Yuan, L. Zhu, J. Song, L. Nie, BadCM: Invisible Backdoor Attack against Cross-Modal Learning, 
			<i>IEEE Transactions on Image Processing (<b>TIP</b>)</i>, 33: 2558-2571, <b>2024</b>. [<a href="https://ieeexplore.ieee.org/document/10478868" target="_blank">Link</a>][<a href="https://github.com/xandery-geek/BadCM" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">Q. Wu, Z. Zhang<sup>&#9993</sup>, Y. Liu, J. Zhang, L. Nie, Contrastive Multi-bit Collaborative Learning for Deep Cross-modal Hashing, 
			<i>IEEE Transactions on Knowledge and Data Engineering (<b>TKDE</b>)</i>, 36 (11): 5835-5848, <b>2024</b>. [<a href="https://ieeexplore.ieee.org/document/10572365" target="_blank">Link</a>][<a href="https://github.com/DarrenZZhang/CMCL" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">H. Luo, Z. Zhang<sup>&#9993</sup>, L. Nie, Contrastive Incomplete Cross-modal Hashing,
			<i>IEEE Transactions on Knowledge and Data Engineering (<b>TKDE</b>)</i>, 36 (11): 5823-5834, <b>2024</b>. [<a href="https://ieeexplore.ieee.org/document/10557685" target="_blank">Link</a>][<a href="https://github.com/DarrenZZhang/CICH" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">Z. Zhang, H. Luo, L. Zhu, G. Lu, H. T. Shen, Modality-Invariant Asymmetric Networks for Cross-Modal Hashing, 
			<i>IEEE Transactions on Knowledge and Data Engineering (<b>TKDE</b>)</i>, vol. 35, no. 5, pp. 5091-5104, <b>2023</b>. [<a href="https://ieeexplore.ieee.org/document/9689994" target="_blank">Link</a>][<a href="https://github.com/E-Galois/MIAN" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">X. Yuan, Z. Zhang<sup>&#9993</sup>, X. Wang, L. Wu, Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval, 
			<i>IEEE Transactions on Information Forensics and Security (<b>TIFS</b>)</i>, 18: 4681-4694, <b>2023</b>. [<a href="https://ieeexplore.ieee.org/document/10189878" target="_blank">Link</a>][<a href="https://github.com/xandery-geek/SAAT" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">Z. Zhang<sup>&#9993</sup>, X. Wang, G. Lu, F. Shen, L. Zhu, Targeted Attack of Deep Hashing via Prototype-supervised Adversarial Networks, 
			<i>IEEE Transactions on Multimedia (<b>TMM</b>)</i>, vol. 24, pp. 3392-3404, <b>2022</b>. [<a href="https://ieeexplore.ieee.org/document/9488305" target="_blank">Link</a>][<a href="https://github.com/xunguangwang/ProS-GAN_Trans" target="_blank">Code</a>]</li>
	</ol> 
	<br><b>Conference paper</b>:</br>
		<ol class="work-list" reversed>
		<li style="margin-bottom: 10px">W. Meng, Y. Luo, X. Li, D. Jiang, Z. Zhang<sup>&#9993</sup>, PolaFormer: Polarity-aware Linear Attention for Vision Transformers, 
			in Proc. of <i>The Thirteenth International Conference on Learning Representations (<b>ICLR</b>)</i>, <b>2025</b>.
		[<a href="https://openreview.net/forum?id=kN6MFmKUSK" target="_blank">Link</a>][<a href="https://github.com/ZacharyMeng/PolaFormer" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">C. Jiang, F. Zhu, X. Chen, J. Zhu, B. Zheng, Y. Wang, Z. Zhang<sup>&#9993</sup>, DMLoRA: Dynamic Multi-Subspace Low-Rank Adaptation, 
			in Proc. of <i>The Web Conference (<b>WWW</b>)</i>, <b>2025</b>.
		[<a href="https://github.com/MobiusDai/DMLoRA" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">Z. Bian, C. Jiang, F. Zhu, Z. Zhang<sup>&#9993</sup>, Selective Multi-grained Alignment for Text-Video Retrieval,
			in Proc. of <i>The Web Conference (<b>WWW</b>)</i>, <b>2025</b>.
		[<a href="https://github.com/bzy-source/SMA" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">W. Li, Z. Zhang<sup>&#9993</sup>, X. Lan<sup>&#9993</sup>, D. Jiang, Transferable Adversarial Face Attack with Text Controlled Attribute, 
			in Proc. of <i>The 39th AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</i>, <b>2025</b>. 
		[<a href="https://arxiv.org/abs/2412.11735" target="_blank">Link</a>][<a href="https://github.com/DarrenZZhang/AAAI25-TCA2" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">X. Guan, Y. Wang, Y. Zhang, Z. Zhang<sup>&#9993</sup>, Y. Zhang<sup>&#9993</sup>, OT-StainNet: Optimal Transport-Driven Semantic Matching for Weakly Paired H&E-to-IHC Stain Transfer, 
			in Proc. of <i>The 39th AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</i>, <b>2025</b>.
		[<a href="https://openreview.net/pdf?id=40IoWBYaTf" target="_blank">Link</a>][<a href="https://github.com/xxx" target="_blank">Code</a>]
		<li style="margin-bottom: 10px">H. Luo, Z. Zhang<sup>&#9993</sup>, Y. Luo, Exploiting Descriptive Completeness Prior for Cross Modal Hashing with Incomplete Labels,
			in Proc. of <i>The Thirty-eighth Annual Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</i>, <b>2024</b>. [<a href="https://neurips.cc/virtual/2024/poster/94194" target="_blank">Link</a>][<a href="https://github.com/E-Galois/PCRIL" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">Y. Liu, J. Wen<sup>&#9993</sup>, C. Liu, X. Fang<sup>&#9993</sup>, Z. Li, Y. Xu, Z. Zhang<sup>&#9993</sup>, Language-Driven Cross-Modal Classifier for Zero-Shot Multi-Label Image Recognition,
			in Proc. of <i>The Forty-first International Conference on Machine Learning (<b>ICML</b>)</i>, <b>2024</b>. [<a href="https://icml.cc/virtual/2024/poster/32917" target="_blank">Link</a>][<a href="https://github.com/yic20/CoMC" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">J. Xu, Y. Ren, X. Wang, L. Feng, Z. Zhang, G. Niu, X. Zhu, Investigating and Mitigating the Side Effects of Noisy Views for Self-Supervised Clustering Algorithms in Practical Multi-View Scenarios,
			in Proc. of <i>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</i>, <b>2024</b>.
			[<a href="https://arxiv.org/abs/2303.17245" target="_blank">Link</a>][<a href="https://github.com/DarrenZZhang/CVPR24-MVCAN" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">Y. Mo, F. Nie, P. Hu, H. T. Shen, Z. Zhang<sup>&#9993</sup>, X. Wang<sup>&#9993</sup>, X. Zhu<sup>&#9993</sup>, Self-supervised Heterogeneous Graph Learning: a Homogeneity and Heterogeneity Perspective, 
			in Proc. of <i>The Twelfth International Conference on Learning Representations (<b>ICLR</b>)</i>, <b>2024</b>. 
			[<a href="https://openreview.net/forum?id=3FJOKjooIj" target="_blank">Link</a>][<a href="https://github.com/YujieMo/HERO" target="_blank">Code</a>]</li>
		<li style="margin-bottom: 10px">X. Wang&sup1;, Z. Zhang&sup1;<sup>&#9993</sup>, B. Wu, F. Shen, G. Lu, Prototype-supervised Adversarial Network for Targeted Attack of Deep Hashing, 
			in Proc. of <i>IEEE Computer Society Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</i>, pp. 16357-16366, <b>2021</b>. 
			[<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Prototype-Supervised_Adversarial_Network_for_Targeted_Attack_of_Deep_Hashing_CVPR_2021_paper.html" target="_blank">Link</a>][<a href="https://github.com/xunguangwang/ProS-GAN" target="_blank">Code</a>]</li>
	</ol>
</div>

<div id="Professional"></div></br>
	<p style="margin-bottom: 15px"><font size = "4.5" face = "Verdana" color="Brown"><b>Professional Activities</b></font></p>
	<b><font color="#660000">Journal Editorial Board Membership</font></b></br>
	<ul class="work-list">
	       <li><b>Associate Editor</b>: IEEE Trans. on Information Forensics & Security (TIFS, 2025 - ), IEEE Trans. on Affective Computing (TAFFC, 2023 - ), 
		       IEEE Journal of Biomedical and Health Informatics (JBHI, 2022 - ), and Expert Systems with Applications (ESWA, 2022--2024).</li>
	       <li><b>Editorial Board Member</b>: Information Fusion (INFFUS, 2022 - ), and Information Processing & Management (IP&M, 2020 - ).</li>
	       <li><b>Early Career Advisory Board Member</b>: IEEE/CAA Journal of Automatica Sinica (JAS, CCF T1, 2021--2025).</li>
	       <li><b>Guest Editors</b>: IEEE JBHI (2025--2026), IEEE JBHI (2023--2024), Information Sciences (2024--2025), Information Fusion (2023--2024), IEEE JBHI (2020--2022), IP&M (2021--2022), IP&M (2020--2021), and etc.</li>
	       <li><b>On-going CFP</b>:</br>
		[1] IEEE JBHI, Special Issue on "AI-based Solutions for Dietary Assessment: Tackling Accuracy, Efficiency, and Personalization Challenges", 2025. [<a href="https://www.embs.org/jbhi/wp-content/uploads/sites/18/2024/12/JBHI_CFP.pdf"  target="_blank">Link</a>]</br>
	</ul>

	<br><b><font color="#660000">Conference Technical Program Committee</font></b></br>
	<ul class="work-list">
	<li><b>Organizing Committee</b>: Tutorial Chair and PhD School Chair for ADMA <a href="https://adma2023.uqcloud.net/index.html"  target="_blank">2023</a>; Organization Chair for VCC 2023; 
		TPC Chair for EAI IoTCare 2024; Chair for EAI IoTCare <a href="https://iotcare.eai-conferences.org/2023/"  target="_blank">2023</a>; Publicity Chair for SSRA 2022; 
		TPC Chair for ACM ICMR 2022 <a href="https://cszhangzheng.github.io/AL4MUR"  target="_blank">Special Session</a>;
		TPC Chair for EAI IoTCare <a href="https://iotcare.eai-conferences.org/2021/"  target="_blank">2021</a>;
		Ph.D. School Chair for ACMM Asia <a href="https://mmasia2021.uqcloud.net/"  target="_blank">2021</a>;
		Publication Chair for ADMA <a href="https://adma2020.nuit.edu.cn/committees-2/index.htm"  target="_blank">2020</a>;
		Special Issue Chair for EAI ICMTEL <a href="https://icmtel.eai-conferences.org/2021/organizing-committee/"  target="_blank">2021</a>.</li>
	<li><b>AC/SPC</b>: ICML, NeurIPS, CVPR, ICLR, ACM MM, AAAI, IJCAI, CIKM, ICME, ECAI, APWeb-WAIM, ADMA, etc.</li>
	<li><b>PC/Reviewer</b>: CVPR, ICCV, ECCV, ICLR, NeurIPS, ICML, ACMM, AAAI, IJCAI, MICCAI, ICDM, ICME, etc.</li>
	<li><b>Recognition</b>: ECCV'20 (Outstanding Reviewer Award), ECAI'23 (Early Bird Award), etc. </li>
	</ul>

	<br><b><font color="#660000">Journal Reviewer</font></b> (20+ IEEE/ACM Trans.): </br>
      <ul class="work-list">
	      <li><b>Reviewer</b>: IEEE TPAMI, IJCV, IEEE TIP, TKDE, TIFS, TSC, TNNLS, TMM, TSP, TMI, TAC, TCYB, TSMCA, TCCN, TCSVT, TII, TASLP, TIM, THMS, TBME, TITS, TCBB, TBD, TETCI, ACM CSUR, ToIT, TOMM, TKDD, TWEB, etc. </li>
	      <li><b>Recognition</b>: IEEE TMI (Distinguished Reviewer), IEEE TNNLS (Outstanding Reviewer), Elsevier IPM (Outstanding Reviewer), etc.
       </ul> 

      <div id="Group"></div></br>
	<p style="margin-bottom: 15px"><font size = "4.5" face = "Verdana" color="Brown"><b>Team</b></font></p>
	<b>Current:</b>
	<ul>
		<li>Yulin Zhao (PhD, 2025- )</li>
		<li>Junchen Hao (PhD, 2025- )</li>
		<li>Ziyi Bian (MS&PhD, 2023-2025- )</li>
		<li><a href="https://github.com/MobiusDai" target="_blank">Cong Jiang</a> (PhD, 2024- )</li>
		<li><a href="https://scholar.google.com/citations?user=OknyWS0AAAAJ&hl=zh-CN" target="_blank">Xianchao Guan</a> (PhD, 2024- )</li>
		<li><a href="https://scholar.google.com/citations?user=PROJfigAAAAJ&hl=en" target="_blank">Wenjue He</a> (MS&PhD, 2022-2024- )</li>
		<li>Ziquan Yu (PhD, 2023- )</li>
		<li><a href="https://scholar.google.com/citations?user=93imFwsAAAAJ&hl=zh-CN" target="_blank">Wenyun Li</a> (PhD, 2023- )</li>
		<li><a href="https://scholar.google.com/citations?user=cZOoYMkAAAAJ&hl=en" target="_blank">Weikang Meng</a> (PhD, 2023- )</li>
		<li>Yishu Liu (PhD with Prof. Lu, 2022- )</li>
		<li>Zijun Xiong (MS, 2024-2027)</li>
		<li>Liangyu Huo (MS, 2024-2027)</li>
		<li>Yubo Cui (MS, 2024-2027)</li>
		<li>Zhijing Huang (MS, 2023-2026)</li>
		<li><a href="https://scholar.google.com.hk/citations?user=TyV8ilIAAAAJ&hl=zh-CN" target="_blank">Zengyang Che</a> (MS, 2022-2025)</li>
	</ul>
	<b>Alumni</b> (2019- )</br>
	<ul>
		<li><a href="https://github.com/xiaomi1024" target="_blank">Mixiao Hou</a> (PhD with Prof. Lu, 2019-2024): Researcher@Second Academy of Astronautics</li>
		<li>Bingzhi Chen (PhD with Prof. Lu, 2017-2022): Researcher@SCNU (Talent Programme)</li>
		<li><a href="https://scholar.google.com/citations?user=4am2MOoAAAAJ&hl=en" target="_blank">Yingjian Li</a> (PhD with Prof. Lu, 2018-2023): AP@HIT (Weihai)</li>
		<li><a href="https://github.com/sky-wz" target="_blank">Qingpeng Wu</a> (MS, 2022-2025): Tencent</li>
		<li>Songling Chen (MS, 2022-2025): GF 601</li>
		<li><a href="https://github.com/wydhhn" target="_blank">Han Lin</a> (MS, 2022-2025): Pony AI Inc.</li>
		<li><a href="http://www.xandery.top/" target="_blank">Xu Yuan</a> (MS, 2021-2024): PhD student@PolyU</li>
		<li><a href="https://github.com/JalinWang" target="_blank">Jianning Wang</a> (MS with Prof. Lu, 2021-2024): Alibaba Group</li>
		<li><a href="https://scholar.google.com/citations?user=2JpKVjcAAAAJ&hl=en" target="_blank">Junfeng An</a> (MS, 2021-2024): Start-up</li>
		<li>Haoyang Luo (MS, 2021-2024): PhD@CityU</li>
		<li>Shuguang Zhao (MS, 2020-2023): Huawei</li>
		<li><a href="https://scholar.google.com/citations?user=KNdj9HMAAAAJ&hl=zh-CN" target="_blank">Xunguang Wang</a> (MS, 2019/09-2021/12):
			PhD@HKUST</li>
		<li>Wenjue He (BS, 2021-2022): MS@HIT</li>
		<li>Keyi Han (BS, 2021-2022): PhD@CAS</li>
		<li>Junfeng An (BS, 2020-2021): MS@HIT</li>
		<li>Haoyang Luo (BS, 2020-2021): MS@HIT</li>
		<li>Jianning Wang (BS, 2020-2021): MS@HIT</li>
	</ul>

	<footer class="site-footer">
	<div class="units-row">
		<div class="unit-50">
			<p>Dr. Zheng Zhang @ HITSZ</p>
		</div>
	</div>
	</footer>
</div>
</main>


</body>

  <script type="text/javascript" src="./asset/jquery.js"></script>
  <script type="text/javascript">
    //Check Nevigation status
    var toggle = true;

    // Nevigation Menu
    $('.btnImg').click(function() {
      if (toggle) {
        $('.btnImg').css("border", "1px solid #b0ccf3");
        toggle = false
      } else {
        $('.btnImg').css("border", "1px solid transparent");
        toggle = true;
      }
      $(".show").slideToggle(300);
    })

    // Window Changes
    $(window).resize(function() {
      // Acquire Window Width
      var windSize = $(window).width();

      if (windSize > 576) {
        $(".show").slideDown(0);
      } else {
        $(".show").slideUp(0);
      }
    });

    // Anchor Controal Funciton
    function finishJump() {
      // Adjust to 30px to avoid blocking 
      var scrollTop = document.documentElement.scrollTop || window.pageYOffset || document.body.scrollTop;
      document.documentElement.scrollTop = scrollTop - 50
      // Close Nevigation
      // Acquire Window Width
      var windSize = $(window).width();
      console.log("windSize:", windSize)
      if (windSize < 576) {
        $('.btnImg').css("border", "1px solid #b0ccf3");
        toggle = false
        $(".show").slideToggle(300);
      }
    }
    // <li><a id="BiographyNav">Biography</a></li>
    // <li><a id="RecentNav">What's New</a></li>
    // <li><a id="PublicationsNav">Publications</a></li>
    // <li><a id="ProfessionalNav">Professional Activities</a></li>
   // <li><a id="GroupNav">Group</a></li>
    // To Top of this page
    $("#logo").click(function() {
      document.documentElement.scrollTop = 0
    });
    $("#BiographyNav").click(function() {
      $("#Biography")[0].scrollIntoView()
      finishJump()
    });
    $("#PublicationsNav").click(function() {
      $("#Publications")[0].scrollIntoView()
      // Post-jump adjustment
      finishJump()
    });
    $("#ProfessionalNav").click(function() {
      $("#Professional")[0].scrollIntoView()
      // Post-jump adjustment
      finishJump()
    });
   $("#GroupNav").click(function() {
      $("#Group")[0].scrollIntoView()
      // Post-jump adjustment
      finishJump()
    });
  </script>

  <style type="text/css" media="screen">
    * {
      margin: 0;
      padding: 0;
      list-style-type: none;
    }

    /* header */
    .page-header {
      margin-top: 58px;
    }

    .container {
      max-width: 1140px;
      margin: 0 auto;
    }

    .c-nav .show {
      display: inline-block;
    }

    .c-nav .hiden {
      display: none;
    }

   /* Framework */
    .main-container {
      width: 100%;
      background-color: rgba(255, 255, 255, 0.8);
      position: relative;
      top: 40px;
      padding: 30px;
    }

    /* Introduction */
    .content-container {
      width: 100%;
      text-align: center;
      display: flex;
      flex-direction: row;
      justify-content: space-around;
      flex-wrap: wrap;
    }

    .content-container-left {
      display: inline-block;
      width: 57%;
      min-width: 250px;
      margin: 20px 0;
    }

    .content-container-right {
      display: inline-block;
      width: 40%;
      min-width: 250px;
      margin: 20px 0;
     /* transform:translateY(-25px) */
    }

    /* navigation */
    .c-nav {
      width: 100%;
      background-color: #E5E7E9;
      position: fixed;
      top: 0rem;
      z-index: 100;
    }

    .c-nav .navFlex {
      display: flex;
      display: -webkit-flex;
      justify-content: space-between;
      -webkit-justify-content: space-between;
      align-items: center;
      -webkit-align-content: center;
      color: #E5E7E9;
    }

    .c-nav ul {
      list-style: none;
      margin-bottom: 0px;
      padding-left: 0px;
    }

    .c-nav ul li {
      padding: 15px 0px 15px 0px;
      margin-left: 30px;
      display: inline-block;
    }

    .c-nav ul li a {
      color: black;
      padding-bottom: 2px;
      text-decoration: none;
      border-bottom: 3px solid transparent;
    }

    .c-nav ul li a:hover {
      border-bottom: 3px solid #e4c17e;
    }

    .c-nav .logo {
      margin-left: 20px;
      height: 40px;
    }

    .c-nav .btnImg {
      height: 20px;
      width: 25px;
      padding: 3px 8px 3px 8px;
      box-sizing: content-box;
      border: 1px solid transparent;
    }

    @media screen and (max-width:1200px) {
      .c-nav ul li {
        margin-left: 20px;
      }
    }

    @media screen and (max-width:992px) {
      .c-nav ul li {
        margin-left: 10px;
      }
    }

    @media screen and (max-width:768px) {

      .c-nav ul li:nth-child(4),
      .c-nav ul li:nth-child(5) {
        display: none;
      }
    }

    @media screen and (max-width:576px) {
      .c-nav {
        /* background-color: rgba(176, 204, 243, 0.8); */
        background-color: white;
        padding: 10px 0px 10px 0px;
        opacity: 0.9;
      }

      .c-nav .navFlex {
        flex-wrap: wrap;
        font-size: 20px;
        justify-content: space-between;
      }

      .c-nav .logo {
        height: 30px;
      }

      .c-nav ul li {
        padding-top: 10px;
        margin-left: 0px;
        display: block;
      }

      .c-nav ul li a {
        border-bottom: 3px solid transparent;
      }

      .c-nav ul a:hover {
        border-bottom: 3px solid #e4c17e;
      }

      .c-nav ul li:nth-child(4),
      .c-nav ul li:nth-child(5) {
        display: block;
      }

      .c-nav .hiden {
        display: block;
      }

      .c-nav .show {
        width: 100%;
        font-size: 14px;
        text-align: center;
        display: none;
      }
    }

    ol li {
      list-style-type: decimal;
      list-style-position: inside;
    }

    ul li {
      list-style-type: disc;
      margin-left: 20px;
    }
  </style>

</html>
